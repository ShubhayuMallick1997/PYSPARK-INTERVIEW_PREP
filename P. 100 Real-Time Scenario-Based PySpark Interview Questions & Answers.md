# Real-Time Scenario-Based PySpark Interview Questions & Answers

### üßë‚Äçüíª Data Ingestion & Processing

**1. How did you handle schema evolution in PySpark when reading data from Snowflake or S3?**
Schema evolution is handled using the `mergeSchema` option (for formats like Parquet). In Snowflake, we dynamically infer schema and use external metadata tables to map and adjust columns. We also write code to detect new columns and add default values during transformation.

**2. What would you do if one API out of 10 fails in a data pipeline?**
We implement try-except blocks around each API call and use logging to capture failures. Failed APIs are added to a retry queue. We also design the DAG to be idempotent so re-running only retries the failed API.

**3. How do you read large files from S3 in chunks using PySpark?**
We partition large files either by time-based folders or split them using S3 Select or pre-processing jobs. PySpark reads in parallel using `.read.format().load()` and we tune the number of partitions using `spark.sql.files.maxPartitionBytes`.

**4. Explain how you created dynamic partitioning while writing data to S3.**
We use `partitionBy("column_name")` while writing DataFrames. For dynamic partitions, we extract the partition columns at runtime based on file content.

**5. How do you validate the structure and content of incoming data files before processing?**
We implement schema validation using StructType. We validate row counts, nulls, duplicates, and date formats before transformation.

---

### ‚öôÔ∏è PySpark Performance Tuning

**6. How did you identify a slow PySpark job in production and what steps did you take to optimize it?**
We used Spark UI to identify long stages and wide transformations. After identifying shuffle-heavy operations, we introduced broadcast joins, tuned partition counts, and cached intermediate DataFrames.

**7. What is data skew, and how have you resolved it in your project?**
Data skew is when one key has significantly more records, leading to unbalanced tasks. We resolved it using salting, i.e., adding a random number to the skewed key to distribute data evenly.

**8. Describe a situation where repartitioning or coalescing made a difference in job performance.**
In one case, 200 partitions were generated by default, slowing down small-file writes. We used `.coalesce(20)` to reduce the overhead.

**9. Have you used broadcast joins in a production scenario? When and why?**
Yes, when joining a large dataset with a small lookup table (<10MB). Broadcasting avoided shuffle and sped up the join operation.

**10. How do you monitor Spark job performance on EMR?**
We use Spark UI, Ganglia, CloudWatch, and EMR job history logs to monitor job runtime, memory usage, GC, and failed tasks.

---

### üíß PySpark Transformations & ETL Logic

**11. How did you implement complex joins and aggregations in PySpark efficiently?**
We used broadcast joins where applicable, cached intermediate DataFrames, and ensured columns were indexed properly using `.select()` to reduce shuffles.

**12. Explain a real-life use case where you had to unnest deeply nested JSON.**
We processed REST API responses with nested arrays. We used `explode()` and `withColumn()` to flatten nested structures and extracted key-value pairs recursively.

**13. How did you create a PySpark pipeline to process data from multiple sources like APIs and databases?**
We used Python requests to pull from APIs, JDBC to read from DBs, and Spark transformations to normalize and merge the data.

**14. Share a scenario where you used window functions to solve a business problem.**
In a churn analysis, we used `row_number()` to rank the latest transactions per customer and `lag()` to compare activities across time.

**15. How do you track and handle duplicate records during ETL processing?**
We use `dropDuplicates()` or window functions with `row_number()` to retain the latest record. We also log duplicates for analysis.

---

### üß† Error Handling & Debugging

**16. What kind of exceptions have you commonly faced in PySpark jobs and how did you fix them?**
Common errors include schema mismatches, null pointer exceptions, and out-of-memory errors. We add extensive logging, null checks, and use `try-except` blocks.

**17. How do you implement retry mechanisms for failed jobs in Airflow?**
We configure retries in DAG definition (`retries=3`, `retry_delay=5min`). We also use alerting with Slack/Email.

**18. Share a scenario where Spark UI helped you debug a failing job.**
In one job, long GC times were visible in Spark UI. We increased executor memory and tuned GC settings to fix it.

**19. How do you capture bad records or corrupt files while reading large datasets?**
We use `badRecordsPath` for CSV/JSON and parse data in try-except blocks for custom formats. Invalid records are logged separately.

**20. What are some logging strategies you follow for PySpark code in production?**
We use `log4j` for structured logging and external logging tools like CloudWatch. Logs are structured with job ID, step name, and timestamp.

---

# Real-Time PySpark Interview Scenarios and Answers

---

### **21. How do you handle schema evolution in PySpark when reading from a data source like Parquet or Snowflake?**

**Answer:** PySpark allows schema evolution using options like `mergeSchema` when reading Parquet files. For example:

```python
spark.read.option("mergeSchema", "true").parquet("path")
```

For Snowflake, we often retrieve the latest schema by querying `INFORMATION_SCHEMA.COLUMNS` and dynamically constructing a DataFrame schema. Alternatively, we use Delta Lake with schema evolution enabled.

---

### **22. Describe a situation where you had to optimize a PySpark job. What steps did you take?**

**Answer:** In a previous project, we had long-running Spark jobs due to skewed joins and large shuffles. To optimize:

* We identified skewed keys using `groupBy().count().orderBy()`.
* Applied **salting** to distribute keys.
* Used **broadcast joins** where possible.
* Repartitioned RDDs before shuffle-heavy operations.
* Cached intermediate results that were reused.

---

### **23. How do you manage job failures and retries in a PySpark + Airflow pipeline?**

**Answer:**

* We configure retries using `retries` and `retry_delay` in Airflow DAGs.
* Within PySpark, we use `try-except` blocks to catch exceptions and log failures.
* Use `on_failure_callback` in Airflow to notify via Slack or email.
* Enable EMR step retry settings via configurations.

---

### **24. Explain how you monitored performance bottlenecks in your PySpark application.**

**Answer:**

* Used Spark UI to examine **stages**, **tasks**, and **executor logs**.
* Checked for **long-running stages**, **shuffle read/write sizes**, and **GC time**.
* Used `.explain(True)` to understand logical and physical plans.
* Enabled `spark.sql.adaptive.enabled=true` for AQE.

---

### **25. In your experience, how have you handled job dependencies in Airflow with PySpark jobs?**

**Answer:**

* We use Airflow's `PythonOperator` or `BashOperator` to trigger PySpark scripts.
* DAGs are structured with proper task dependencies using `set_upstream()` or `>>` operators.
* Also used `ExternalTaskSensor` for inter-DAG dependencies.
* Employed `TriggerRule` to manage conditional paths.

---

### **26. How do you manage configuration for multiple environments (dev/stage/prod) in PySpark projects?**

**Answer:**

* Maintain environment-specific YAML or JSON config files.
* Load them dynamically in Spark scripts.
* Use environment variables via `os.environ` or Airflow `Variables`.
* Also use `SecretsManager` for credentials in AWS-based environments.

---

### **27. Give an example of how you dealt with skewed data in PySpark joins.**

**Answer:**

* Identified skew using key distribution metrics.
* Introduced **salting**: modified keys on one side (e.g., add random suffix) and exploded keys on the other side to match.
* Used **broadcast joins** if one table was small.
* Applied `repartition()` before the join to ensure better distribution.

---

### **28. What is your approach when your PySpark job is consuming too much memory?**

**Answer:**

* Repartitioned data to reduce size per partition.
* Used narrow transformations instead of wide ones.
* Tuned `spark.executor.memory`, `spark.executor.cores`, `spark.sql.shuffle.partitions`.
* Used `persist(StorageLevel.DISK_ONLY)` to offload from memory.

---

### **29. How do you handle late arriving data or backfills in a streaming pipeline?**

**Answer:**

* Used **watermarking** with event time in Structured Streaming:

```python
.withWatermark("event_time", "1 hour")
```

* Handled late events in downstream logic.
* For backfills, triggered batch jobs with past data range and separate S3 path.

---

### **30. Describe a scenario where you needed to ensure idempotency in your PySpark job.**

**Answer:**

* Idempotency was critical while writing output to S3 or Snowflake.
* Achieved it by using `overwrite` mode and **partitioned output** (e.g., by date).
* Used job run identifiers or timestamps to separate directories.
* Maintained audit logs in a control table to track processed data.

---

Great! Here's **Batch 4 (Q31‚Äì40)** of real-time **PySpark interview scenario-based questions with detailed answers**:

---

### **31. How do you handle missing or null values in PySpark DataFrames?**

**Answer:**

Handling nulls depends on context:

* **Drop rows with nulls:**

  ```python
  df.na.drop()
  ```

  Options:

  * `df.na.drop(how="any")` ‚Äì drop if any column has null
  * `df.na.drop(how="all")` ‚Äì drop only if all columns are null

* **Fill nulls with default values:**

  ```python
  df.na.fill({"age": 0, "name": "unknown"})
  ```

* **Filter out nulls explicitly:**

  ```python
  df.filter(df["age"].isNotNull())
  ```

Use method based on business logic (e.g., for ML pipelines, fill with mean/median using `imputer`).

---

### **32. How do you optimize PySpark joins in real-time pipelines?**

**Answer:**

Optimization techniques:

* **Broadcast smaller table:**

  ```python
  from pyspark.sql.functions import broadcast
  df1.join(broadcast(df2), "id")
  ```

* **Repartition by join key to reduce shuffle:**

  ```python
  df1.repartition("id").join(df2.repartition("id"), "id")
  ```

* **Avoid skewed joins (use salting):** Add/mod hash key to balance partition sizes.

* **Use appropriate join type (e.g., avoid full outer joins unless necessary).**

* Enable join hints if supported:

  ```python
  df1.join(df2.hint("broadcast"), "id")
  ```

---

### **33. How do you debug job failure in EMR while running a PySpark job?**

**Answer:**

Steps:

* **Check Spark UI:** Look for stages, DAGs, and failed task logs.
* **Review EMR logs in S3:** EMR logs go to `s3://<log-bucket>/logs/`.
* **Enable detailed logs:**

  * Set `spark.eventLog.enabled=true`
  * Use `log4j.properties` for custom logging
* **Common errors:**

  * OutOfMemory ‚Üí tweak executor memory
  * Data Skew ‚Üí salting
  * Missing files/schemas ‚Üí validate paths and schema before execution

---

### **34. What is the use of `.persist()` vs `.cache()` in a real job?**

**Answer:**

* `.cache()` is a shortcut for `.persist(StorageLevel.MEMORY_AND_DISK)`.
* Use **`.persist()`** when you want control over storage level (e.g., disk-only, off-heap).
* Use **`.cache()`** when RDD/DataFrame is used multiple times and fits in memory.
* Helps in iterative algorithms (ML, graph).

```python
df.persist(StorageLevel.MEMORY_AND_DISK)
```

---

### **35. How do you test PySpark code locally before deployment?**

**Answer:**

* Use **`local[*]` mode** for SparkSession:

  ```python
  spark = SparkSession.builder.master("local[*]").appName("test").getOrCreate()
  ```

* Use **pytest** or **unittest** with:

  * Temporary DataFrames
  * Sample input files in `/tmp`
  * Mocking APIs/snowflake with `responses` or test doubles

* Leverage `.collect()` and `.show()` for validation

---

### **36. How do you manage schema evolution in real-time pipelines (e.g., Snowflake source)?**

**Answer:**

* Use **schema inference** carefully ‚Äî schema drift can break ETL.
* Strategy:

  * Read schema separately via sampling
  * Validate new schema using `.schema.json()`
  * Write schema comparison logic
  * Add new columns as nullable or handle using `selectExpr` with `IF EXISTS`

Tools:

* Delta Lake schema evolution (`mergeSchema`)
* Glue Schema Registry for tracking

---

### **37. Have you implemented dynamic partitioning in PySpark? How?**

**Answer:**

Yes. To enable dynamic partitioning:

```python
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")
df.write.partitionBy("year", "month").mode("overwrite").parquet("s3://bucket/output/")
```

* It partitions by `year` and `month` dynamically
* Helps reduce scan space for queries (e.g., Athena, Hive)

---

### **38. How do you integrate external APIs into your PySpark job?**

**Answer:**

* Use Python‚Äôs `requests` or `httpx` inside UDFs **with caution** (not recommended for heavy API calls).

Best practice:

* **Extract API data separately** via:

  * Standalone Python script using `requests`
  * Save API data as JSON/CSV to S3
  * Ingest using PySpark `spark.read.json("s3://...")`

Reason:

* Spark isn‚Äôt meant for row-wise API I/O ‚Äì it slows down massively.

---

### **39. What‚Äôs your approach to version-controlling PySpark jobs?**

**Answer:**

* Use **Git** to maintain:

  * PySpark job scripts
  * Config files (YAML/JSON for param settings)
  * Airflow DAGs
* Use branches for feature and hotfixes
* Enforce PRs with code review
* Use `.gitignore` for logs, `__pycache__`, temp data
* Store job artifacts (egg/whl) for CI/CD pipelines

---

### **40. How do you monitor your PySpark jobs in production?**

**Answer:**

* **Monitoring Tools:**

  * **Spark UI** for execution details
  * **CloudWatch (AWS)** for EMR cluster resource metrics
  * **Ganglia** / **Prometheus + Grafana**
  * **Airflow UI** for DAG and task status
* Log all:

  * Data stats (record count, nulls)
  * Execution time
  * Error stack traces

Use logging libraries:

```python
import logging
logging.basicConfig(level=logging.INFO)
```

---
Here‚Äôs **Batch 5 (Q41‚Äì50)** of real-time **PySpark interview scenario-based questions** with detailed answers:

---

### **41. How do you handle large file ingestion (100+ GB) efficiently in PySpark?**

**Answer:**

Handling large files in PySpark:

* **Use partitioning:**

  ```python
  df = spark.read.option("header", "true").csv("s3://bucket/largefile/", multiLine=True)
  df = df.repartition(100)
  ```

  Increases parallelism and reduces shuffle.

* **Use appropriate file format:** Prefer **Parquet** or **ORC** over CSV/JSON for columnar compression.

* **Enable predicate pushdown**:

  ```python
  spark.conf.set("spark.sql.parquet.filterPushdown", "true")
  ```

* **Tune Spark config:** Use `spark.executor.memory`, `spark.sql.shuffle.partitions`.

---

### **42. What are common causes of memory issues in PySpark jobs?**

**Answer:**

* **Too much data in a single partition** (data skew)
* **Caching huge DataFrames** without enough memory
* **Improper joins** (e.g., wide join + no broadcast)
* **Expensive UDFs** that create high GC overhead
* **No checkpointing** in iterative jobs (e.g., ML)
* **Unbounded data growth** due to unfiltered joins or aggregations

**Fixes:**

* Repartition/coalesce
* Use broadcast join
* Avoid unnecessary `.collect()` or `.cache()`
* Increase executor memory or use auto-scaling on EMR

---

### **43. Explain your deployment process for PySpark jobs.**

**Answer:**

Typical deployment steps:

1. **Code pushed to Git**
2. **CI/CD via Jenkins/GitHub Actions:**

   * Code linting & testing
   * Build `egg` or `whl` packages
3. **Deploy on EMR using Airflow DAG**:

   * BashOperator or EMROperator triggers job
   * Logs stored in CloudWatch/S3
4. **Monitoring:** Slack/email alerts on failure

---

### **44. How do you implement SCD (Slowly Changing Dimensions) in PySpark?**

**Answer:**

Two types handled commonly:

* **SCD Type 1 (overwrite):**

  * Just overwrite old record with new

  ```python
  df.write.mode("overwrite").saveAsTable("dim_table")
  ```

* **SCD Type 2 (history tracking):**

  * Add new record with `current_flag`, `effective_date`, `expiry_date`
  * Use `window` and `when` to close old records:

    ```python
    from pyspark.sql.window import Window
    from pyspark.sql.functions import when, col, lead

    w = Window.partitionBy("id").orderBy("effective_date")
    df = df.withColumn("expiry_date", lead("effective_date").over(w))
    ```

---

### **45. How do you build reusable code for PySpark projects?**

**Answer:**

* **Modular functions** for cleaning, transformation, loading

* Store in Python files/modules

* Example:

  ```python
  # transformations.py
  def clean_names(df):
      return df.withColumn("name", upper(col("name")))
  ```

* Import in jobs:

  ```python
  from transformations import clean_names
  ```

* Store configs in YAML/JSON and parse in jobs

---

### **46. How do you secure secrets (like API keys or DB credentials) in PySpark workflows?**

**Answer:**

Best practices:

* **Use AWS Secrets Manager** or environment variables
* Use Airflow‚Äôs `Variable` or `Connections` feature
* Avoid hardcoding in scripts

Example:

```python
import boto3, json
secrets = boto3.client('secretsmanager')
secret_val = secrets.get_secret_value(SecretId="snowflake_conn")["SecretString"]
credentials = json.loads(secret_val)
```

---

### **47. How do you perform job retries for failures in PySpark?**

**Answer:**

Two ways:

* **Within PySpark using try-except + retry logic:**

  ```python
  for _ in range(3):
      try:
          run_my_job()
          break
      except Exception as e:
          print("Retrying...", e)
  ```

* **External orchestration tools (Airflow)**: Use `retries` parameter:

  ```python
  BashOperator(task_id='spark_job', retries=3, retry_delay=timedelta(minutes=5))
  ```

---

### **48. Explain how you handled schema mismatch while reading data.**

**Answer:**

Schema mismatch scenarios:

* **Extra/missing columns:** Use `schema` parameter to enforce expected structure
* **Different column types:** Use `cast()` or `selectExpr()`

Preventive approach:

```python
schema = StructType([...])
df = spark.read.schema(schema).csv("path")
```

Use `badRecordsPath` to isolate failures:

```python
.option("badRecordsPath", "s3://bucket/badrecords/")
```

---

### **49. What are your go-to commands during debugging?**

**Answer:**

* `.printSchema()` ‚Äì check structure
* `.show(n)` ‚Äì peek into records
* `.explain()` ‚Äì show physical and logical plan
* `.count()` ‚Äì to confirm row count
* Spark UI ‚Üí Stages, Executors, DAG, logs
* Custom `logging` module for capturing detailed logs

---

### **50. What steps do you take before promoting a PySpark job to production?**

**Answer:**

Checklist:

1. **Unit tested logic** (pytest/unittest)
2. **Run on lower env (dev/staging)** with test data
3. **Validate performance and memory use**
4. **Setup alerts** for failures (email, Slack)
5. **Code review and approval**
6. **Use parameterized configs**
7. **Ensure versioning and rollback plan**

---

Here‚Äôs **Batch 6 (Q51‚Äì60)** of real-time **PySpark scenario-based interview questions** with detailed answers:

---

### **51. How do you handle null or missing values in PySpark DataFrames?**

**Answer:**

PySpark provides multiple ways:

* **Drop nulls**:

  ```python
  df.dropna()  # drops rows with any null
  df.dropna(how='all')  # drops rows only if all are null
  ```

* **Fill nulls**:

  ```python
  df.fillna({'age': 0, 'name': 'unknown'})
  ```

* **Filter out**:

  ```python
  df.filter(df['column'].isNotNull())
  ```

* **Impute (MLlib)**:

  ```python
  from pyspark.ml.feature import Imputer
  imputer = Imputer(inputCols=['age'], outputCols=['age_filled']).setStrategy("mean")
  result = imputer.fit(df).transform(df)
  ```

---

### **52. How do you track lineage of transformations in PySpark?**

**Answer:**

You can use:

* **Spark‚Äôs `.explain()`** method:

  ```python
  df.explain(True)
  ```

* **Spark UI** ‚Üí shows DAG of stages/tasks

* **Data lineage tools**: Like Apache Atlas, Databricks Unity Catalog, Amundsen

* **Custom metadata tracking**: logging input/output schema, columns, timestamps manually

---

### **53. What is your approach to optimize a job that takes 3 hours to run?**

**Answer:**

Step-by-step:

1. **Check the DAG and stage-level execution** in Spark UI
2. **Look for skewed partitions**
3. **Broadcast small datasets for joins**
4. **Use Parquet instead of CSV**
5. **Repartition based on volume** (`repartition`, `coalesce`)
6. **Avoid wide transformations or caching huge DataFrames**
7. **Review UDFs** ‚Äì replace with native functions or `pandas_udf`
8. **Use `.persist(StorageLevel.MEMORY_AND_DISK)`** instead of `.cache()` if large

---

### **54. How do you update specific rows in a DataFrame?**

**Answer:**

Since DataFrames are immutable, use `when()` and `otherwise()`:

```python
from pyspark.sql.functions import when

df = df.withColumn(
    "salary",
    when(df["department"] == "HR", df["salary"] * 1.1).otherwise(df["salary"])
)
```

For full-table updates (like in SCD), use overwrite or merge logic using Delta Lake or Snowflake.

---

### **55. Describe how you implemented job orchestration using Airflow.**

**Answer:**

Typical setup:

1. **Define DAG** with scheduling, retries, notifications
2. Use **`BashOperator`** or **`EMROperator`**:

   ```python
   BashOperator(
       task_id="run_spark",
       bash_command="spark-submit s3://bucket/code.py",
       retries=2
   )
   ```
3. Set **dependencies** between tasks using `>>`
4. Track logs via Airflow UI / S3 / CloudWatch
5. Notify via email/Slack on failure

---

### **56. How do you read data from Snowflake in PySpark?**

**Answer:**

Use the Snowflake connector:

```python
sfOptions = {
  "sfURL": "account.region.snowflakecomputing.com",
  "sfUser": "username",
  "sfPassword": "password",
  "sfDatabase": "DB",
  "sfSchema": "SCHEMA",
  "sfWarehouse": "WAREHOUSE"
}

df = spark.read.format("snowflake").options(**sfOptions).option("dbtable", "TABLE").load()
```

To improve security, pull secrets from **Secrets Manager** or **Airflow connections**.

---

### **57. How do you version data in PySpark pipelines?**

**Answer:**

* Add **timestamp column** or partition by `load_date`

* Maintain versions using folder names:

  ```
  s3://bucket/data/2024-01-01/
  s3://bucket/data/2024-01-02/
  ```

* If using **Delta Lake**, versioning is built-in:

  ```python
  df.write.format("delta").save("s3://bucket/path")
  spark.read.format("delta").option("versionAsOf", 3).load("s3://bucket/path")
  ```

---

### **58. What‚Äôs your strategy for joining 3+ large datasets?**

**Answer:**

* Join two at a time to control shuffle

* Use **broadcast join** on smallest table

* Repartition if skewed joins:

  ```python
  df1 = df1.repartition("key")
  df2 = df2.repartition("key")
  ```

* Avoid redundant columns early

* Cache intermediate joins if reused

Example:

```python
df_final = df1.join(broadcast(df2), "id").join(df3, "id2")
```

---

### **59. How do you test PySpark code locally before running on a cluster?**

**Answer:**

* Use **`local[*]`** mode:

  ```python
  spark = SparkSession.builder.master("local[*]").appName("test").getOrCreate()
  ```

* Unit test with `pytest` or `unittest`

* Sample test:

  ```python
  def test_add_column():
      df = spark.createDataFrame([(1,)], ["id"])
      result = add_column(df)
      assert "new_col" in result.columns
  ```

* Use small test datasets in CSV/JSON

---

### **60. How do you automate schema validation in your pipeline?**

**Answer:**

Use either:

1. **Manual validation** using `df.schema == expected_schema`

2. **PyDeequ** for automated assertions:

   ```python
   from pydeequ.checks import Check
   check = Check(spark, CheckLevel.Error, "DataCheck")
   check.hasSize(lambda x: x > 0).isComplete("id")
   ```

3. **Custom code** to check:

   * Nulls
   * Column data types
   * Unique constraints
   * Allowed values (enum)

---


Great! Here's **Batch 7 (Q61‚Äì70)** of **real-time PySpark scenario-based interview questions** with detailed answers:

---

### **61. How do you handle late-arriving data in a PySpark pipeline?**

**Answer:**

Late-arriving data refers to records that arrive after their expected processing window. You can handle it using:

* **Watermarking** (in Structured Streaming):

  ```python
  df_with_watermark = df.withWatermark("event_time", "10 minutes")
  ```

* **Partition overwrite**:

  * If data is daily partitioned, reprocess affected partitions (e.g., overwrite `"dt=2024-07-10"`).

* **Delta Lake Time Travel**:

  * Compare versions and reconcile late-arriving data.

* **Maintain a retry or correction pipeline**:

  * Trigger backfills when data is detected as delayed.

---

### **62. How do you maintain data consistency between multiple Spark jobs?**

**Answer:**

Best practices:

1. **Atomic writes** using overwrite or copy-on-write methods.
2. **Avoid overlapping job windows**.
3. Use **transactional tables** (like Delta Lake).
4. Employ **lock mechanisms** using a flag file or database lock table.
5. Implement **idempotent writes** to avoid duplicates:

   * Use `dropDuplicates` or insert-only-if-not-exist logic.

---

### **63. How do you ensure schema evolution is handled in your PySpark job?**

**Answer:**

* Use **schema merging** for formats like Parquet:

  ```python
  df = spark.read.option("mergeSchema", "true").parquet("s3://bucket/data")
  ```

* Define a **schema registry** or maintain expected schemas in JSON.

* On evolution:

  * Add new columns with default values
  * Use `.selectExpr()` to reorder columns if needed
  * Use tools like **AWS Glue Schema Registry** or **Confluent Schema Registry** for streaming

---

### **64. How do you debug out-of-memory (OOM) errors in EMR Spark jobs?**

**Answer:**

Steps:

1. **Check Spark UI** ‚Äì find stages with long GC or shuffle

2. Increase memory:

   ```bash
   --executor-memory 8G --driver-memory 4G
   ```

3. **Use `persist(StorageLevel.DISK_ONLY)`** to avoid filling RAM

4. **Avoid wide transformations** like groupByKey

5. **Reduce parallelism** or increase partitions

6. Enable GC logging for investigation

---

### **65. What is the best way to deal with skewed joins in PySpark?**

**Answer:**

* **Broadcast small table**:

  ```python
  df1.join(broadcast(df2), "id")
  ```

* **Salting technique**:

  ```python
  from pyspark.sql.functions import rand, concat
  skewed_df = df.withColumn("salt", (rand()*10).cast("int"))
  ```

* **Repartition before join**:

  ```python
  df.repartition("id")
  ```

* **Use bucketing** or partitioning at source

---

### **66. How do you run multiple PySpark jobs in parallel?**

**Answer:**

* **From Airflow** ‚Äì define parallel DAG branches

* **Using Threads/Process in Python** (less preferred):

  ```python
  from threading import Thread
  Thread(target=job1).start()
  ```

* **Via separate Spark sessions (on different clusters)**

* **Submit in parallel using shell**:

  ```bash
  spark-submit job1.py & spark-submit job2.py &
  ```

Ensure resource isolation between jobs.

---

### **67. How do you handle corrupted records while reading data in PySpark?**

**Answer:**

* Use **`badRecordsPath`**:

  ```python
  spark.read.option("badRecordsPath", "s3://bad/path").json("s3://input/")
  ```

* Use `PERMISSIVE` mode (default), or switch to `DROPMALFORMED`:

  ```python
  spark.read.option("mode", "DROPMALFORMED").json("s3://input/")
  ```

* Log, isolate, or alert on corrupted files

---

### **68. How do you handle SCD (Slowly Changing Dimension) in PySpark?**

**Answer:**

* **SCD Type 1** (overwrite):

  * Join staging and dimension, update directly

* **SCD Type 2** (history):

  * Insert new record with updated value and timestamp
  * Expire old record by setting `is_current = false`

Delta Lake or Snowflake make SCD handling easier with MERGE:

```sql
MERGE INTO dim_table USING staging ON id = staging.id
WHEN MATCHED THEN UPDATE SET ...
WHEN NOT MATCHED THEN INSERT ...
```

---

### **69. How do you optimize performance when writing millions of rows to S3 in PySpark?**

**Answer:**

* Use **Parquet** or **ORC**, not CSV

* **Repartition** to control output files:

  ```python
  df.repartition(100).write.parquet("s3://bucket/output/")
  ```

* Enable **snappy compression**

* Avoid `.coalesce(1)` unless necessary

* Use **EMRFS consistent view** or **S3A** connector

* Optimize `spark.sql.shuffle.partitions`

---

### **70. How do you handle JSON files with complex nested structures?**

**Answer:**

1. **Read normally**:

   ```python
   df = spark.read.json("s3://bucket/data.json", multiLine=True)
   ```

2. **Use `explode()`** to flatten arrays:

   ```python
   from pyspark.sql.functions import explode
   df = df.withColumn("items", explode(df["order.items"]))
   ```

3. **Access nested fields**:

   ```python
   df.select("order.customer.name", "order.items.product_id")
   ```

4. Use `.schema.json()` to inspect structure

---

Perfect! Here's **Batch 8 (Q71‚Äì80)** of **real-time PySpark scenario-based interview questions with detailed answers**:

---

### **71. What are best practices for handling null values in PySpark DataFrames?**

**Answer:**

* **Detect nulls**:

  ```python
  df.filter(df["column"].isNull())
  ```

* **Drop nulls**:

  ```python
  df.dropna()  # or df.dropna(subset=["col1", "col2"])
  ```

* **Fill nulls with defaults**:

  ```python
  df.fillna({"age": 0, "name": "unknown"})
  ```

* **Use `when().otherwise()` for custom logic**:

  ```python
  from pyspark.sql.functions import when
  df.withColumn("status", when(df["score"].isNull(), "NA").otherwise("Valid"))
  ```

* Always **understand domain impact** before replacing or dropping.

---

### **72. How do you track job execution in production?**

**Answer:**

* **Spark UI** (via YARN or Spark History Server)
* **Airflow UI** if jobs are DAG-based
* Use **Spark listeners** for fine-grained monitoring
* Log **job start/end time, row count, failure reason**
* Use custom **logging frameworks (e.g., log4j/slf4j)** for traceability
* Export metrics to **Prometheus + Grafana** or **AWS CloudWatch**

---

### **73. What is speculative execution in Spark and how do you use it?**

**Answer:**

Speculative execution is used to **re-run slow tasks** on other nodes to prevent bottlenecks due to **stragglers**.

* Enabled with:

  ```python
  spark.conf.set("spark.speculation", "true")
  spark.conf.set("spark.speculation.quantile", 0.75)
  spark.conf.set("spark.speculation.multiplier", 1.5)
  ```

* Helps when some nodes are slow or have bad disks.

* Use cautiously in **write-heavy jobs**, as it may duplicate writes unless idempotent.

---

### **74. How do you handle schema mismatch between two DataFrames during a union?**

**Answer:**

* Ensure **column order and types** match.

* Use `select()` to align schemas explicitly:

  ```python
  df1 = df1.select("id", "name", "age")
  df2 = df2.select("id", "name", "age")
  union_df = df1.union(df2)
  ```

* If columns differ:

  ```python
  from pyspark.sql.functions import lit
  df2 = df2.withColumn("new_col", lit(None).cast("String"))
  ```

* Optionally, use **Delta or Iceberg** to auto-evolve schemas.

---

### **75. What are common issues while reading large CSV files and how do you fix them?**

**Answer:**

* **Header inconsistencies**:

  * Use `header=True`

* **Encoding issues**:

  * Set `encoding="utf-8"` or `ISO-8859-1`

* **Corrupt lines**:

  * Use `mode="PERMISSIVE"` or `badRecordsPath`

* **Data skew** due to small files:

  * Merge small files or repartition

* **Empty strings interpreted as nulls**:

  * Use `nullValue` or `treatEmptyValuesAsNulls`

---

### **76. How do you create and use temporary views in PySpark SQL?**

**Answer:**

* **Create view**:

  ```python
  df.createOrReplaceTempView("orders")
  ```

* **Query**:

  ```python
  spark.sql("SELECT * FROM orders WHERE price > 100")
  ```

* Temporary views are **session-scoped**.

* Use **global\_temp** if needed across sessions:

  ```python
  df.createOrReplaceGlobalTempView("orders")
  ```

---

### **77. How do you implement logging in your PySpark applications?**

**Answer:**

* **Use `log4j`** via SparkContext:

  ```python
  log4jLogger = sc._jvm.org.apache.log4j
  logger = log4jLogger.LogManager.getLogger(__name__)
  logger.info("This is a log message")
  ```

* Or **Python logging** module:

  ```python
  import logging
  logging.basicConfig(level=logging.INFO)
  logging.info("Starting job...")
  ```

* Log job start, config, step outputs, errors, and metrics.

---

### **78. How do you debug shuffle issues in Spark jobs?**

**Answer:**

* Check **shuffle read/write size** in Spark UI.

* **Too many stages or long task durations** indicate shuffle problems.

* Reduce shuffle:

  * Use `reduceByKey` instead of `groupByKey`
  * Repartition data wisely

* Use:

  ```python
  spark.conf.set("spark.sql.shuffle.partitions", 100)
  ```

* Enable **adaptive query execution (AQE)**:

  ```python
  spark.conf.set("spark.sql.adaptive.enabled", "true")
  ```

---

### **79. How do you join a streaming DataFrame with a static DataFrame?**

**Answer:**

* Static DF:

  ```python
  dim_df = spark.read.csv("path/to/static.csv", header=True)
  ```

* Streaming DF:

  ```python
  stream_df = spark.readStream.schema(schema).json("s3://stream/path")
  ```

* Join:

  ```python
  joined = stream_df.join(broadcast(dim_df), "id")
  ```

* Static DF must fit in memory (broadcast recommended)

---

### **80. How do you write a PySpark job to read from one S3 path and write into another, with data transformation?**

**Answer:**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, upper

spark = SparkSession.builder.appName("ETL Job").getOrCreate()

# Read from S3
df = spark.read.csv("s3://input-bucket/data.csv", header=True)

# Transformation
df_transformed = df.withColumn("name_upper", upper(col("name")))

# Write to S3
df_transformed.write.mode("overwrite").parquet("s3://output-bucket/cleaned/")
```

* Use IAM roles or keys for auth
* Enable S3 optimizations like `fs.s3a.multipart.uploads.enabled`

---

Great! Here's **Batch 9 (Q81‚Äì90)** of **real-time PySpark scenario-based interview questions with detailed answers**:

---

### **81. How do you process multiple file formats (CSV, JSON, Parquet) in a single PySpark job?**

**Answer:**

You can read different formats individually and then normalize them for processing:

```python
csv_df = spark.read.option("header", True).csv("path/to/csv")
json_df = spark.read.json("path/to/json")
parquet_df = spark.read.parquet("path/to/parquet")

# Ensure schema consistency
common_df = csv_df.select("id", "name", "age") \
    .union(json_df.select("id", "name", "age")) \
    .union(parquet_df.select("id", "name", "age"))
```

üëâ Use `select()` or `withColumn()` to align column names and data types.

---

### **82. What is a skewed join and how do you resolve it in PySpark?**

**Answer:**

A **skewed join** happens when one key has **disproportionately high records**, causing one executor to do most of the work.

**Solution**:

* **Salting technique**:
  Add a random prefix or suffix to keys:

  ```python
  from pyspark.sql.functions import rand, concat_ws
  df1 = df1.withColumn("join_key_salted", concat_ws("_", df1["join_key"], (rand()*10).cast("int")))
  df2 = df2.withColumn("join_key_salted", concat_ws("_", df2["join_key"], lit("0")))  # Replicate for all
  ```

* Or use:

  * **Broadcast join** for small tables
  * **Repartitioning** before join

---

### **83. How do you ensure data quality before writing into a target?**

**Answer:**

Use these steps:

* **Null/empty checks**:

  ```python
  df.filter(df["col1"].isNull())
  ```

* **Duplicate detection**:

  ```python
  df.dropDuplicates()
  ```

* **Schema validation** using `schema.simpleString()` or comparing with expected schema

* **Threshold alerts** on record counts

* **Data profiling** with min/max/count of fields

---

### **84. How to design a reusable PySpark function for transformations?**

**Answer:**

Wrap logic into a Python function:

```python
def clean_customer_data(df):
    return df.withColumn("name", upper(col("name"))) \
             .withColumn("is_valid", when(col("age") > 0, True).otherwise(False))
```

Usage:

```python
df_cleaned = clean_customer_data(df)
```

Reusable for testing, modular pipelines, and parameterization.

---

### **85. How do you deal with corrupt or malformed records while reading data in PySpark?**

**Answer:**

Set **read modes**:

```python
df = spark.read.option("mode", "PERMISSIVE").json("path/")
```

Modes:

* `"PERMISSIVE"` ‚Äì corrupt records go to `_corrupt_record` column
* `"DROPMALFORMED"` ‚Äì drops bad records
* `"FAILFAST"` ‚Äì throws an error

Use `badRecordsPath` to store rejected rows for debugging:

```python
.option("badRecordsPath", "s3://bucket/errors/")
```

---

### **86. What is the difference between DataFrame and Dataset in Spark?**

**Answer:**

| Feature     | DataFrame (PySpark)              | Dataset (Scala/Java)                  |
| ----------- | -------------------------------- | ------------------------------------- |
| Language    | Python, Scala, Java              | Scala, Java only                      |
| Type Safety | No (Python is dynamically typed) | Yes (strong compile-time type checks) |
| Performance | High with Catalyst               | Same                                  |
| Usability   | More Pythonic in PySpark         | Type-safe in Scala/Java               |

üëâ PySpark **does not support Datasets**, only DataFrames and RDDs.

---

### **87. Explain a pipeline you worked on with real-time + batch ingestion.**

**Answer:**

*Example real-world scenario*:

* **Batch**:

  * Source: Snowflake and S3 ‚Üí PySpark on EMR
  * Processing: Aggregations, joins, filters
  * Output: Final data written to S3 ‚Üí queried with Athena

* **Streaming**:

  * Source: Kafka (sales transactions)
  * Processing: PySpark Structured Streaming
  * Output: Written to S3 (Parquet) every 5 mins
  * Combined with batch data in downstream analytics

Tools:

* Airflow DAGs
* AWS Glue catalog
* Data quality checks and alerts on anomalies

---

### **88. How do you track the lineage of your data across multiple PySpark jobs?**

**Answer:**

* Use **Airflow task DAGs** with clear naming

* Include **job metadata logging**:

  * Source ‚Üí Target mapping
  * Timestamps
  * Row counts

* Integrate with tools like:

  * **Apache Atlas**
  * **OpenLineage**
  * **Great Expectations** for data validation

Custom lineage logging:

```python
logger.info(f"Ingested {df.count()} rows from source_X at {timestamp}")
```

---

### **89. What is AQE (Adaptive Query Execution)?**

**Answer:**

**AQE** optimizes query execution **dynamically at runtime** based on actual stats.

Features:

* Dynamically adjusts **number of shuffle partitions**
* Converts **sort-merge joins to broadcast joins**
* Skips **unnecessary partitions**

Enable it:

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

Improves performance without changing code.

---

### **90. What is the role of the Catalyst Optimizer in PySpark?**

**Answer:**

Catalyst Optimizer is Spark‚Äôs internal engine that:

* Optimizes query plans via:

  * Constant folding
  * Predicate pushdown
  * Reordering joins
  * Pruning columns
* Converts logical plan ‚Üí optimized physical plan

Example:

```python
df.select("name").filter(col("age") > 18)
```

Even if written in reverse order, Spark rearranges to filter first ‚Üí then select.

‚úÖ Boosts performance by reducing I/O and shuffle.

---

Perfect! Here's **Batch 10 (Q91‚Äì100)** of **real-time PySpark scenario-based interview questions with detailed answers**:

---

### **91. How do you handle schema evolution in PySpark when the source schema changes frequently?**

**Answer:**

Schema evolution refers to managing changing fields (new, missing, or modified).

**Techniques:**

* Use **`mergeSchema=True`** when reading Parquet:

  ```python
  df = spark.read.option("mergeSchema", "true").parquet("s3://bucket/path/")
  ```

* For **JSON** or **CSV**, infer schema with:

  ```python
  spark.read.option("inferSchema", "true")
  ```

* Build **robust ETL** by:

  * Reading schema dynamically
  * Using try/except or `schema_of_json()`
  * Defaulting missing fields with `withColumn` + `lit(None)`

---

### **92. How do you debug memory issues in a PySpark job?**

**Answer:**

Memory issues are common with large datasets. Steps to debug:

* Check **Spark UI** ‚Üí Storage & Executors tab

* Analyze **stage execution time** and **shuffle read/write size**

* Tune:

  * `spark.executor.memory`
  * `spark.driver.memory`
  * `spark.sql.shuffle.partitions`

* Use:

  * `persist()` with proper storage level
  * `repartition()` or `coalesce()` to balance data
  * Avoid large `collect()` calls

---

### **93. How do you monitor your PySpark jobs in production?**

**Answer:**

Monitoring techniques:

* **Spark UI** (default 4040 port or Yarn history server)

* **Airflow DAG** logs (for job orchestration)

* **Cloud-native logs**:

  * EMR + CloudWatch
  * Databricks logs
  * S3 log sinks

* Use **metrics**:

  * Number of processed rows
  * Execution time per stage
  * Memory usage
  * Retry/failure count

* Set alerts (e.g., Slack, Email) on failure using Airflow `on_failure_callback`.

---

### **94. How do you write unit tests for PySpark code?**

**Answer:**

Use `pytest` or `unittest` with `SparkSession` fixture:

```python
from pyspark.sql import SparkSession
import pytest

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[1]").appName("Test").getOrCreate()

def test_uppercase(spark):
    df = spark.createDataFrame([("john",)], ["name"])
    df2 = df.withColumn("name", upper(col("name")))
    assert df2.collect()[0]["name"] == "JOHN"
```

Tools:

* `chispa` for comparing DataFrames
* `pytest-spark`

---

### **95. How do you handle null or missing values in PySpark?**

**Answer:**

Common techniques:

* Drop rows:

  ```python
  df.dropna()
  ```

* Fill nulls:

  ```python
  df.fillna({"age": 0, "name": "Unknown"})
  ```

* Replace with logic:

  ```python
  df.withColumn("age", when(col("age").isNull(), 0).otherwise(col("age")))
  ```

* Check nulls:

  ```python
  df.filter(col("col_name").isNull())
  ```

---

### **96. How do you join two large datasets efficiently in PySpark?**

**Answer:**

* Use **broadcast join** when one table is small:

  ```python
  from pyspark.sql.functions import broadcast
  df1.join(broadcast(df2), "id")
  ```

* Repartition before join:

  ```python
  df1.repartition("join_key").join(df2.repartition("join_key"), "join_key")
  ```

* Avoid shuffles:

  * Pre-partition the data
  * Use sorted merge join if applicable

* Cache if used repeatedly

---

### **97. How do you manage job dependencies in a DAG pipeline?**

**Answer:**

Using **Apache Airflow**:

* Each task = PySpark job (via BashOperator or EMRStepOperator)
* Define `upstream` and `downstream` tasks:

```python
t1 >> t2 >> t3  # t2 runs after t1
```

* Handle failure:

  * Retry policies
  * Email on failure
  * Break pipeline if dependency fails

Also use status files or markers in S3 (e.g., `_SUCCESS` files) in ad-hoc workflows.

---

### **98. Explain checkpointing in PySpark Streaming.**

**Answer:**

**Checkpointing** saves:

* Metadata
* Offsets
* DAG lineage

It is essential for **fault-tolerance** in streaming apps.

Use:

```python
stream_df.writeStream.option("checkpointLocation", "s3://path/checkpoint/").start()
```

Types:

* **Metadata Checkpointing** (structured streaming)
* **RDD Checkpointing** for stateful ops:

  ```python
  sc.setCheckpointDir("/tmp/checkpoints")
  rdd.checkpoint()
  ```

---

### **99. How do you convert nested JSON to a flat DataFrame in PySpark?**

**Answer:**

Use `explode()` + `select` + `col()` chaining:

```python
from pyspark.sql.functions import col, explode

df = spark.read.json("nested.json")

df = df.select("id", explode("items").alias("item")) \
       .select("id", "item.name", "item.price")
```

Or use:

* `schema_of_json()` + `from_json()` for dynamic schema
* `flatten()` logic for deeply nested data

---

### **100. What are the different types of window functions in PySpark?**

**Answer:**

Window functions allow operations **over a group of rows** (window).

Types:

* **Ranking functions**:

  * `row_number()`, `rank()`, `dense_rank()`
* **Aggregation**:

  * `sum()`, `avg()`, `min()`, `max()`, `count()`
* **Lead/Lag**:

  * `lag()`, `lead()` ‚Äì access previous/next row

Example:

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

windowSpec = Window.partitionBy("dept").orderBy("salary")
df.withColumn("rank", row_number().over(windowSpec))
```

---



