# **50 Top PySpark Interview Questions** 
###— divided by topic — to help you **crack interviews from beginner to expert level**:

---

### ✅ **1. PySpark Basics**

1. What is PySpark, and how is it different from Apache Spark?
2. What are the benefits of using PySpark?
3. What are the different components of the Spark ecosystem?
4. Explain the architecture of Spark.
5. What is the role of the driver and executor in Spark?

---

### ✅ **2. SparkContext & SparkSession**

6. What is `SparkContext` and why is it important?
7. What is `SparkSession`? How is it different from `SparkContext`?
8. How do you configure Spark settings using `SparkConf`?
9. How do you stop a SparkSession programmatically?

---

### ✅ **3. RDDs (Resilient Distributed Datasets)**

10. What is an RDD in PySpark?
11. How do you create an RDD?
12. What are transformations and actions in RDD?
13. What is the difference between `map()` and `flatMap()`?
14. How does lazy evaluation work in Spark?
15. What is the difference between `cache()` and `persist()`?

---

### ✅ **4. DataFrames & Datasets**

16. What is a DataFrame in PySpark?
17. How do you create a DataFrame from RDD or file?
18. What are some common DataFrame operations?
19. What is the difference between RDD and DataFrame?
20. How is schema inferred or manually defined in a DataFrame?

---

### ✅ **5. DataFrame Transformations**

21. How do you add or modify a column in PySpark?
22. How do you filter rows in a DataFrame?
23. Explain the use of `withColumn()`, `drop()`, and `alias()`.
24. What are the common string and date functions in PySpark?
25. How do you handle missing values in PySpark?

---

### ✅ **6. Aggregations & Grouping**

26. How does `groupBy()` work in PySpark?
27. What are window functions? Give examples.
28. How do you calculate rolling aggregates (e.g., moving average)?
29. Explain the difference between `agg()` and individual aggregations like `sum()` or `count()`.
30. What is pivoting and when would you use it?

---

### ✅ **7. Joins in PySpark**

31. What types of joins are supported in PySpark?
32. What is a broadcast join and when do you use it?
33. How do you handle data skew in joins?
34. What’s the difference between inner, outer, and semi joins?
35. How do you optimize joins in PySpark?

---

### ✅ **8. UDFs (User Defined Functions)**

36. What is a UDF in PySpark?
37. How do you register a UDF?
38. What are the performance issues with UDFs?
39. What is a `pandas_udf` and how is it different from a regular UDF?

---

### ✅ **9. File Handling & Storage**

40. How do you read/write files in formats like CSV, JSON, Parquet?
41. What are partitioning and bucketing? How do they help performance?
42. How do you read/write data from S3 in PySpark?

---

### ✅ **10. Advanced Topics**

43. What is Catalyst Optimizer?
44. What is the Tungsten engine?
45. What are accumulators and broadcast variables?
46. How do you monitor and debug a PySpark job?
47. How do you integrate PySpark with Airflow?
48. How do you connect to Snowflake from PySpark?
49. How do you schedule PySpark jobs in production?
50. Explain your most challenging PySpark project and how you optimized it.

---


